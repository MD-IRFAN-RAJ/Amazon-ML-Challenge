{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1800c278-c596-4913-b8fc-66822c3c40b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° Downloading all images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [1:00:11<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è 1 images failed ‚Äî placeholders created.\n",
      "‚úÖ Total images in folder: 72288\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# === CONFIG ===\n",
    "train_csv_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\train.csv\"\n",
    "download_folder = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\images\\train\"\n",
    "MAX_WORKERS = 60          # ‚ö° Optimized for Windows\n",
    "TIMEOUT = 10              # seconds per request\n",
    "RETRY_LIMIT = 3\n",
    "\n",
    "# === PREP ===\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "image_links = train_df[\"image_link\"].tolist()\n",
    "\n",
    "# === SESSION SETUP ===\n",
    "session = requests.Session()\n",
    "adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.mount(\"http://\", adapter)\n",
    "\n",
    "def create_placeholder(path):\n",
    "    \"\"\"Create a gray placeholder for missing image.\"\"\"\n",
    "    img = Image.new(\"RGB\", (224, 224), color=(200, 200, 200))\n",
    "    img.save(path)\n",
    "\n",
    "def download_one(url):\n",
    "    \"\"\"Download a single image with retries.\"\"\"\n",
    "    filename = os.path.basename(url)\n",
    "    save_path = os.path.join(download_folder, filename)\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        return None  # already downloaded\n",
    "\n",
    "    for attempt in range(RETRY_LIMIT):\n",
    "        try:\n",
    "            response = session.get(url, timeout=TIMEOUT)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return None\n",
    "        except Exception:\n",
    "            pass  # retry quietly\n",
    "\n",
    "    # if failed after all retries\n",
    "    create_placeholder(save_path)\n",
    "    return url\n",
    "\n",
    "# === DOWNLOAD LOOP ===\n",
    "failed = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(download_one, url) for url in image_links]\n",
    "    for f in tqdm(as_completed(futures), total=len(futures), desc=\"‚ö° Downloading all images\"):\n",
    "        result = f.result()\n",
    "        if result:\n",
    "            failed.append(result)\n",
    "\n",
    "# === SUMMARY ===\n",
    "if failed:\n",
    "    with open(\"failed_downloads.txt\", \"w\") as file:\n",
    "        file.write(\"\\n\".join(failed))\n",
    "    print(f\"\\n‚ö†Ô∏è {len(failed)} images failed ‚Äî placeholders created.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All images downloaded successfully!\")\n",
    "\n",
    "print(\"‚úÖ Total images in folder:\", len(os.listdir(download_folder)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50dd805f-4e19-4b44-a954-280bcf69e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Missing images: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_csv = pd.read_csv(r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\train.csv\")\n",
    "download_folder = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\images\\train\"\n",
    "\n",
    "all_expected = set(os.path.basename(link) for link in train_csv[\"image_link\"])\n",
    "all_downloaded = set(os.listdir(download_folder))\n",
    "\n",
    "missing = list(all_expected - all_downloaded)\n",
    "print(f\"‚ùå Missing images: {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24671cd4-6031-41ca-b016-465d4c27fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (75000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55858</td>\n",
       "      <td>Item Name: Judee‚Äôs Blue Cheese Powder 11.25 oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n",
       "      <td>30.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292686</td>\n",
       "      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n",
       "      <td>66.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "3      55858  Item Name: Judee‚Äôs Blue Cheese Powder 11.25 oz...   \n",
       "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
       "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
       "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\train.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0d84c4-2a00-4157-9bcf-fadbe77e4c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Exact duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = train_df[train_df.duplicated()]\n",
    "print(\"üîÅ Exact duplicate rows:\", len(duplicate_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06743f55-1007-4acf-aa59-659d655bdbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Duplicate image links: 2175\n"
     ]
    }
   ],
   "source": [
    "dup_images = train_df[train_df.duplicated(subset=[\"image_link\"], keep=False)]\n",
    "print(\"üñºÔ∏è Duplicate image links:\", dup_images[\"image_link\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b142937-1d0f-4938-9a93-d5fc822fed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìú Duplicate catalog_content entries: 93\n"
     ]
    }
   ],
   "source": [
    "dup_texts = train_df[train_df.duplicated(subset=[\"catalog_content\"], keep=False)]\n",
    "print(\"üìú Duplicate catalog_content entries:\", dup_texts[\"catalog_content\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06307e47-f927-4332-932f-917fc6e3d413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded test.csv ‚Äî 75000 rows\n",
      "\n",
      "üöÄ Generating predictions (safe mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:17<00:00, 4239.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Submission saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Rows in submission: 75000 | Expected: 75000\n",
      "‚úÖ Missing / default predictions used: 75000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\"\n",
    "test_csv_path = os.path.join(base_path, \"test.csv\")\n",
    "test_img_folder = os.path.join(base_path, \"data\", \"images\", \"test\")\n",
    "\n",
    "# === LOAD TEST DATA ===\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"‚úÖ Loaded test.csv ‚Äî {len(test_df)} rows\")\n",
    "\n",
    "# === SETTINGS ===\n",
    "DEFAULT_PRED = \"Unknown\"  # fallback label if image missing or unreadable\n",
    "submission_file = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# === PLACEHOLDER MODEL FUNCTION ===\n",
    "# Replace this with your actual model inference function later\n",
    "def predict_image(image_path):\n",
    "    # TODO: replace this logic with your trained model‚Äôs prediction\n",
    "    # For now, it just returns \"SamplePrediction\"\n",
    "    return \"SamplePrediction\"\n",
    "\n",
    "# === RUN INFERENCE SAFELY ===\n",
    "predictions = []\n",
    "\n",
    "print(\"\\nüöÄ Generating predictions (safe mode)...\")\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    img_url = row.get(\"image_link\", \"\")\n",
    "    img_name = os.path.basename(img_url)\n",
    "    img_path = os.path.join(test_img_folder, img_name)\n",
    "\n",
    "    # 1Ô∏è‚É£ If file missing ‚Üí fallback\n",
    "    if not os.path.exists(img_path):\n",
    "        predictions.append(DEFAULT_PRED)\n",
    "        continue\n",
    "\n",
    "    # 2Ô∏è‚É£ Try to load and predict\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            pred = predict_image(img_path)\n",
    "        predictions.append(pred)\n",
    "    except Exception as e:\n",
    "        # If unreadable ‚Üí fallback\n",
    "        predictions.append(DEFAULT_PRED)\n",
    "\n",
    "# === VERIFY PREDICTION LENGTH ===\n",
    "assert len(predictions) == len(test_df), \"‚ùå Mismatch between predictions and test.csv rows!\"\n",
    "\n",
    "# === SAVE SUBMISSION ===\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],  # use correct ID column\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "\n",
    "submission_file = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved successfully: {submission_file}\")\n",
    "print(f\"‚úÖ Rows in submission: {len(submission_df)} | Expected: {len(test_df)}\")\n",
    "print(f\"‚úÖ Missing / default predictions used: {predictions.count(DEFAULT_PRED)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7465564c-02b1-4149-81b2-8062f20c6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sample_id', 'catalog_content', 'image_link'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>Item Name: Rani 14-Spice Eshamaya's Mango Chut...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71hoAn78AW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>Item Name: Natural MILK TEA Flavoring extract ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61ex8NHCIj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>Item Name: Honey Filled Hard Candy - Bulk Pack...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61KCM61J8e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51Ex6uOH7y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>Item Name: McCormick Culinary Vanilla Extract,...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71QYlrOMoS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
       "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
       "2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
       "3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
       "4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n",
       "\n",
       "                                          image_link  \n",
       "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
       "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n",
       "2  https://m.media-amazon.com/images/I/61KCM61J8e...  \n",
       "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...  \n",
       "4  https://m.media-amazon.com/images/I/71QYlrOMoS...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\test.csv\")\n",
    "print(test_df.columns)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "531e4fac-e24f-45aa-8f6a-ce05b56cfd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded train.csv with 75000 rows\n",
      "Train size: 60000, Validation size: 15000\n",
      "\n",
      "üöÄ Training Ridge Regression model...\n",
      "\n",
      "‚úÖ Validation RMSE: 26.721\n",
      "‚úÖ Validation R¬≤: 0.284\n",
      "\n",
      "üíæ Model saved to: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 3: TF-IDF Feature Extraction + Ridge Regression\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "train_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\train.csv\"\n",
    "model_save_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\"\n",
    "\n",
    "# ---------- LOAD DATA ----------\n",
    "train_df = pd.read_csv(train_path)\n",
    "print(f\"‚úÖ Loaded train.csv with {len(train_df)} rows\")\n",
    "\n",
    "# ---------- BASIC CLEANING ----------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# Drop rows with missing price\n",
    "train_df = train_df.dropna(subset=[\"price\"])\n",
    "train_df = shuffle(train_df, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ---------- SPLIT DATA ----------\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"],\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Validation size: {len(X_valid)}\")\n",
    "\n",
    "# ---------- TF-IDF + REGRESSION PIPELINE ----------\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1,2),\n",
    "        stop_words=\"english\"\n",
    "    )),\n",
    "    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "# ---------- TRAIN ----------\n",
    "print(\"\\nüöÄ Training Ridge Regression model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ---------- VALIDATE ----------\n",
    "preds = pipeline.predict(X_valid)\n",
    "rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "r2 = r2_score(y_valid, preds)\n",
    "\n",
    "print(f\"\\n‚úÖ Validation RMSE: {rmse:.3f}\")\n",
    "print(f\"‚úÖ Validation R¬≤: {r2:.3f}\")\n",
    "\n",
    "# ---------- SAVE MODEL ----------\n",
    "joblib.dump(pipeline, model_save_path)\n",
    "print(f\"\\nüíæ Model saved to: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bbe870a-c9f3-4888-bad5-a95f83ea1bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading model and test data...\n",
      "‚úÖ Loaded test.csv ‚Äî 75000 rows\n",
      "\n",
      "üöÄ Generating predictions (safe mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [01:46<00:00, 707.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Submission saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Rows in submission: 75000 | Expected: 75000\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# STEP 4: SAFE SUBMISSION GENERATOR (TF-IDF MODEL)\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ---------- PATHS ----------\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "data_path = os.path.join(base_path, \"data\")  # data folder\n",
    "test_csv_path = os.path.join(data_path, \"test.csv\")  # test.csv inside data/\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")  # model in My/\n",
    "submission_file = os.path.join(base_path, \"submission.csv\")  # output in My/\n",
    "\n",
    "# ---------- LOAD MODEL & TEST DATA ----------\n",
    "print(\"üìÇ Loading model and test data...\")\n",
    "model = joblib.load(model_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"‚úÖ Loaded test.csv ‚Äî {len(test_df)} rows\")\n",
    "\n",
    "# ---------- TEXT CLEANING ----------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "# ---------- PREDICTION ----------\n",
    "predictions = []\n",
    "DEFAULT_PRED = np.nan  # fallback numeric prediction\n",
    "\n",
    "print(\"\\nüöÄ Generating predictions (safe mode)...\")\n",
    "for text in tqdm(test_df[\"catalog_content\"], total=len(test_df)):\n",
    "    try:\n",
    "        if len(text.strip()) == 0:\n",
    "            predictions.append(DEFAULT_PRED)\n",
    "        else:\n",
    "            pred = model.predict([text])[0]\n",
    "            predictions.append(pred)\n",
    "    except Exception:\n",
    "        predictions.append(DEFAULT_PRED)\n",
    "\n",
    "# ---------- FILL ANY MISSING VALUES ----------\n",
    "predictions = [\n",
    "    np.mean([p for p in predictions if not np.isnan(p)]) if np.isnan(p) else p\n",
    "    for p in predictions\n",
    "]\n",
    "\n",
    "# ---------- VERIFY LENGTH ----------\n",
    "assert len(predictions) == len(test_df), \"‚ùå Mismatch between predictions and test.csv rows!\"\n",
    "\n",
    "# ---------- SAVE SUBMISSION ----------\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],  # must match exactly\n",
    "    \"prediction\": predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved successfully: {submission_file}\")\n",
    "print(f\"‚úÖ Rows in submission: {len(submission_df)} | Expected: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da611c35-af9c-444f-98ee-37c46bd9bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rows match: True\n",
      "‚úÖ Unique sample_id: True\n",
      "‚úÖ All sample_ids from test.csv present: True\n",
      "\n",
      "üìä Submission Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>33.806560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>27.051277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>23.675204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>14.558085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>37.686976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id  prediction\n",
       "0     100179   33.806560\n",
       "1     245611   27.051277\n",
       "2     146263   23.675204\n",
       "3      95658   14.558085\n",
       "4      36806   37.686976"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "test_csv = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "submission_csv = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# Load both files\n",
    "test_df = pd.read_csv(test_csv)\n",
    "sub_df = pd.read_csv(submission_csv)\n",
    "\n",
    "# Checks\n",
    "print(\"‚úÖ Rows match:\", len(test_df) == len(sub_df))\n",
    "print(\"‚úÖ Unique sample_id:\", sub_df[\"sample_id\"].is_unique)\n",
    "print(\"‚úÖ All sample_ids from test.csv present:\", \n",
    "      set(test_df[\"sample_id\"]) == set(sub_df[\"sample_id\"]))\n",
    "\n",
    "# Show a preview\n",
    "print(\"\\nüìä Submission Preview:\")\n",
    "display(sub_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36f6b618-6d97-4351-89b5-d6abab23fe87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Training data shape: (75000, 4)\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3      55858  Item Name: Judee‚Äôs Blue Cheese Powder 11.25 oz...   \n",
      "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
      "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
      "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \n",
      "\n",
      "üßπ Cleaning data...\n",
      "‚úÖ Cleaned data shape: (75000, 4)\n",
      "\n",
      "üìä Split complete:\n",
      "Train size: 60000 | Validation size: 15000\n",
      "\n",
      "üî† Vectorizing text using TF-IDF...\n",
      "‚úÖ TF-IDF features: 100000\n",
      "\n",
      "‚öôÔ∏è Training Ridge Regression model...\n",
      "\n",
      "üìà Evaluating model...\n",
      "‚úÖ Validation MAE: 14.129\n",
      "‚úÖ Validation SMAPE: 68.20%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving trained model...\n",
      "‚úÖ Model saved to: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# üìò STEP 3: Train Text-Based Price Prediction Model\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "\n",
    "# === LOAD TRAINING DATA ===\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(\"‚úÖ Training data shape:\", train_df.shape)\n",
    "print(train_df.head())\n",
    "\n",
    "# === BASIC CHECKS ===\n",
    "required_cols = [\"sample_id\", \"catalog_content\", \"image_link\", \"price\"]\n",
    "for col in required_cols:\n",
    "    if col not in train_df.columns:\n",
    "        raise ValueError(f\"‚ùå Missing column: {col}\")\n",
    "\n",
    "# === CLEAN DATA ===\n",
    "print(\"\\nüßπ Cleaning data...\")\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].fillna(\"\")\n",
    "train_df[\"price\"] = pd.to_numeric(train_df[\"price\"], errors=\"coerce\")\n",
    "train_df = train_df.dropna(subset=[\"price\"])\n",
    "train_df = train_df[train_df[\"price\"] > 0]\n",
    "print(\"‚úÖ Cleaned data shape:\", train_df.shape)\n",
    "\n",
    "# === TRAIN-VALIDATION SPLIT ===\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"], train_df[\"price\"],\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Split complete:\")\n",
    "print(f\"Train size: {len(X_train)} | Validation size: {len(X_valid)}\")\n",
    "\n",
    "# === TEXT FEATURE EXTRACTION (TF-IDF) ===\n",
    "print(\"\\nüî† Vectorizing text using TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=100000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_valid_tfidf = tfidf.transform(X_valid)\n",
    "print(\"‚úÖ TF-IDF features:\", X_train_tfidf.shape[1])\n",
    "\n",
    "# === TRAIN RIDGE REGRESSION ===\n",
    "print(\"\\n‚öôÔ∏è Training Ridge Regression model...\")\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# === VALIDATION ===\n",
    "print(\"\\nüìà Evaluating model...\")\n",
    "y_pred = ridge.predict(X_valid_tfidf)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "smape = np.mean(\n",
    "    np.abs(y_pred - y_valid) / ((np.abs(y_pred) + np.abs(y_valid)) / 2)\n",
    ") * 100\n",
    "\n",
    "print(f\"‚úÖ Validation MAE: {mae:.3f}\")\n",
    "print(f\"‚úÖ Validation SMAPE: {smape:.2f}%\")\n",
    "\n",
    "# === PLOT PREDICTION DISTRIBUTION ===\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(y_pred, bins=50, alpha=0.7, label=\"Predicted\")\n",
    "plt.hist(y_valid, bins=50, alpha=0.7, label=\"Actual\")\n",
    "plt.legend()\n",
    "plt.title(\"Predicted vs Actual Price Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "print(\"\\nüíæ Saving trained model...\")\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "joblib.dump({\"model\": ridge, \"vectorizer\": tfidf}, model_path)\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f3bde12-2230-45de-a166-ed1741e71c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Shape: (75000, 4)\n",
      "\n",
      "üßπ Cleaning...\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ‚ö° IMPROVED TEXT-BASED MODEL FOR PRODUCT PRICE PREDICTION\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import re\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"üìÇ Loading training data...\")\n",
    "df = pd.read_csv(train_csv_path)\n",
    "print(\"‚úÖ Shape:\", df.shape)\n",
    "\n",
    "# === CLEAN DATA ===\n",
    "print(\"\\nüßπ Cleaning...\")\n",
    "df[\"catalog_content\"] = df[\"catalog_content\"].fillna(\"\").astype(str)\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736bacbe-4a29-4ead-958a-36e59d3d2e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Training data shape: (75000, 4)\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3      55858  Item Name: Judee‚Äôs Blue Cheese Powder 11.25 oz...   \n",
      "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
      "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
      "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \n",
      "\n",
      "üßπ Cleaning data...\n",
      "‚úÖ Cleaned data shape: (75000, 4)\n",
      "\n",
      "‚öôÔ∏è Extracting meta features...\n",
      "‚úÖ Meta features added: 8\n",
      "\n",
      "üî† Vectorizing text using optimized TF-IDF (‚âà5 min)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:17<00:00, 4204.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train size: 63750 | Valid size: 11250\n",
      "\n",
      "‚öôÔ∏è Training Ridge Regression...\n",
      "\n",
      "üìà Evaluating model...\n",
      "\n",
      "‚úÖ Validation MAE: 12.444\n",
      "‚úÖ Validation SMAPE: 54.45%\n",
      "\n",
      "üíæ Model saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# üöÄ OPTIMIZED PRODUCT PRICE PREDICTION (TEXT-ONLY MODEL)\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "\n",
    "# === LOAD TRAINING DATA ===\n",
    "print(\"üìÇ Loading training data...\")\n",
    "df = pd.read_csv(train_csv_path)\n",
    "print(\"‚úÖ Training data shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# === CLEAN DATA ===\n",
    "print(\"\\nüßπ Cleaning data...\")\n",
    "df[\"catalog_content\"] = df[\"catalog_content\"].fillna(\"\").astype(str)\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"price\"])\n",
    "df = df[df[\"price\"] > 0]\n",
    "print(\"‚úÖ Cleaned data shape:\", df.shape)\n",
    "\n",
    "# === FEATURE ENGINEERING ===\n",
    "def extract_features(text):\n",
    "    text = text.lower()\n",
    "    return {\n",
    "        \"len\": len(text),\n",
    "        \"num_words\": len(text.split()),\n",
    "        \"num_digits\": sum(c.isdigit() for c in text),\n",
    "        \"contains_pack\": int(\"pack\" in text),\n",
    "        \"contains_oz\": int(\"oz\" in text or \"ounce\" in text),\n",
    "        \"contains_lb\": int(\"lb\" in text),\n",
    "        \"contains_ml\": int(\"ml\" in text),\n",
    "        \"contains_kg\": int(\"kg\" in text),\n",
    "    }\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Extracting meta features...\")\n",
    "meta_features = pd.DataFrame([extract_features(t) for t in df[\"catalog_content\"]])\n",
    "print(\"‚úÖ Meta features added:\", meta_features.shape[1])\n",
    "\n",
    "# === TF-IDF VECTORIZE ===\n",
    "print(\"\\nüî† Vectorizing text using optimized TF-IDF (‚âà5 min)...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=30000,        # ‚ö° safe for 8 GB RAM\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_text = tfidf.fit_transform(tqdm(df[\"catalog_content\"], desc=\"TF-IDF progress\"))\n",
    "\n",
    "# === COMBINE TEXT + META ===\n",
    "scaler = StandardScaler()\n",
    "X_meta = scaler.fit_transform(meta_features)\n",
    "X = hstack([X_text, X_meta])\n",
    "\n",
    "# === LOG-TRANSFORM TARGET ===\n",
    "y = np.log1p(df[\"price\"])\n",
    "\n",
    "# === TRAIN-VALID SPLIT ===\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "print(f\"‚úÖ Train size: {X_train.shape[0]} | Valid size: {X_valid.shape[0]}\")\n",
    "\n",
    "# === TRAIN MODEL ===\n",
    "print(\"\\n‚öôÔ∏è Training Ridge Regression...\")\n",
    "ridge = Ridge(alpha=2.0, random_state=42, solver=\"auto\", max_iter=2000)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# === VALIDATION ===\n",
    "print(\"\\nüìà Evaluating model...\")\n",
    "y_pred_log = ridge.predict(X_valid)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_valid_exp = np.expm1(y_valid)\n",
    "\n",
    "mae = mean_absolute_error(y_valid_exp, y_pred)\n",
    "smape = np.mean(\n",
    "    np.abs(y_pred - y_valid_exp) / ((np.abs(y_pred) + np.abs(y_valid_exp)) / 2)\n",
    ") * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Validation MAE: {mae:.3f}\")\n",
    "print(f\"‚úÖ Validation SMAPE: {smape:.2f}%\")\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "joblib.dump({\"model\": ridge, \"vectorizer\": tfidf, \"scaler\": scaler}, model_path)\n",
    "print(f\"\\nüíæ Model saved successfully: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8685470e-73a8-416f-805a-cf6f5dd02e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading test data...\n",
      "‚úÖ Test data shape: (75000, 3)\n",
      "   sample_id                                    catalog_content  \\\n",
      "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
      "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
      "2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
      "3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
      "4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n",
      "\n",
      "                                          image_link  \n",
      "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
      "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n",
      "2  https://m.media-amazon.com/images/I/61KCM61J8e...  \n",
      "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...  \n",
      "4  https://m.media-amazon.com/images/I/71QYlrOMoS...  \n",
      "\n",
      "üíæ Loading saved model...\n",
      "‚úÖ Model and vectorizer loaded successfully!\n",
      "\n",
      "üßπ Cleaning test text...\n",
      "‚öôÔ∏è Extracting meta features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:09<00:00, 8029.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî† Transforming test text with TF-IDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:17<00:00, 4207.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Predicting prices...\n",
      "‚úÖ Predictions generated: 75000\n",
      "\n",
      "üìÑ Building submission file...\n",
      "\n",
      "‚úÖ Submission saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Rows in submission: 75000 | Expected: 75000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100179</td>\n",
       "      <td>15.251376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245611</td>\n",
       "      <td>15.388465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146263</td>\n",
       "      <td>19.602015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95658</td>\n",
       "      <td>6.708650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36806</td>\n",
       "      <td>51.366832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id      price\n",
       "0     100179  15.251376\n",
       "1     245611  15.388465\n",
       "2     146263  19.602015\n",
       "3      95658   6.708650\n",
       "4      36806  51.366832"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# üìò STEP 4: Generate Predictions and Submission CSV\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# === LOAD TEST DATA ===\n",
    "print(\"üìÇ Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(\"‚úÖ Test data shape:\", test_df.shape)\n",
    "print(test_df.head())\n",
    "\n",
    "# === LOAD TRAINED MODEL ===\n",
    "print(\"\\nüíæ Loading saved model...\")\n",
    "bundle = joblib.load(model_path)\n",
    "ridge = bundle[\"model\"]\n",
    "tfidf = bundle[\"vectorizer\"]\n",
    "scaler = bundle[\"scaler\"]\n",
    "print(\"‚úÖ Model and vectorizer loaded successfully!\")\n",
    "\n",
    "# === CLEAN TEST TEXT ===\n",
    "print(\"\\nüßπ Cleaning test text...\")\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].fillna(\"\").astype(str)\n",
    "\n",
    "# === EXTRACT META FEATURES ===\n",
    "def extract_features(text):\n",
    "    text = text.lower()\n",
    "    return {\n",
    "        \"len\": len(text),\n",
    "        \"num_words\": len(text.split()),\n",
    "        \"num_digits\": sum(c.isdigit() for c in text),\n",
    "        \"contains_pack\": int(\"pack\" in text),\n",
    "        \"contains_oz\": int(\"oz\" in text or \"ounce\" in text),\n",
    "        \"contains_lb\": int(\"lb\" in text),\n",
    "        \"contains_ml\": int(\"ml\" in text),\n",
    "        \"contains_kg\": int(\"kg\" in text),\n",
    "    }\n",
    "\n",
    "print(\"‚öôÔ∏è Extracting meta features for test set...\")\n",
    "meta_features_test = pd.DataFrame([extract_features(t) for t in tqdm(test_df[\"catalog_content\"])])\n",
    "X_meta_test = scaler.transform(meta_features_test)\n",
    "\n",
    "# === TF-IDF TRANSFORM ===\n",
    "print(\"\\nüî† Transforming test text with TF-IDF...\")\n",
    "X_text_test = tfidf.transform(tqdm(test_df[\"catalog_content\"], desc=\"TF-IDF progress\"))\n",
    "X_test = hstack([X_text_test, X_meta_test])\n",
    "\n",
    "# === PREDICT PRICES ===\n",
    "print(\"\\nü§ñ Predicting prices...\")\n",
    "y_pred_log = ridge.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)        # reverse log-transform\n",
    "y_pred = np.maximum(y_pred, 0)       # ensure no negatives\n",
    "\n",
    "print(f\"‚úÖ Predictions generated: {len(y_pred)}\")\n",
    "\n",
    "# === BUILD SUBMISSION FILE ===\n",
    "print(\"\\nüìÑ Building submission file...\")\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": y_pred\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved successfully: {submission_path}\")\n",
    "print(f\"‚úÖ Rows in submission: {len(submission)} | Expected: {len(test_df)}\")\n",
    "\n",
    "# === PREVIEW ===\n",
    "display(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068b0b55-ce90-4411-8d10-55d0ae127aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "‚úÖ Test shape: (75000, 3)\n",
      "‚úÖ Submission shape: (75000, 2)\n",
      "\n",
      "üßæ Verifying submission consistency...\n",
      "‚úÖ Same length: True\n",
      "‚úÖ Sample ids match: True\n",
      "‚úÖ No missing predictions: True\n",
      "‚úÖ All positive prices: True\n",
      "\n",
      "üéâ Submission file format looks perfect!\n",
      "\n",
      "üíæ Loading trained model...\n",
      "‚úÖ Model loaded successfully.\n",
      "\n",
      "üßπ Cleaning train data for validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting meta: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:09<00:00, 8065.21it/s]\n",
      "TF-IDF transforming: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:17<00:00, 4266.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Predicting on validation split...\n",
      "\n",
      "üìä Local Validation Results:\n",
      "‚úÖ MAE: 12.444\n",
      "‚úÖ SMAPE: 54.45%\n",
      "\n",
      "üì¶ Submission ready at: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# ‚úÖ Step 4.5: Verify Submission & Compute Local SMAPE\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# === LOAD TRAIN & TEST ===\n",
    "print(\"üìÇ Loading data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "sub_df = pd.read_csv(submission_path)\n",
    "print(\"‚úÖ Train shape:\", train_df.shape)\n",
    "print(\"‚úÖ Test shape:\", test_df.shape)\n",
    "print(\"‚úÖ Submission shape:\", sub_df.shape)\n",
    "\n",
    "# === VERIFY SUBMISSION FILE ===\n",
    "print(\"\\nüßæ Verifying submission consistency...\")\n",
    "checks = {\n",
    "    \"same_length\": len(test_df) == len(sub_df),\n",
    "    \"sample_ids_match\": set(test_df[\"sample_id\"]) == set(sub_df[\"sample_id\"]),\n",
    "    \"no_missing_predictions\": not sub_df[\"price\"].isna().any(),\n",
    "    \"all_positive_prices\": (sub_df[\"price\"] >= 0).all()\n",
    "}\n",
    "\n",
    "for key, val in checks.items():\n",
    "    print(f\"‚úÖ {key.replace('_', ' ').capitalize()}: {val}\")\n",
    "\n",
    "if all(checks.values()):\n",
    "    print(\"\\nüéâ Submission file format looks perfect!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some issues found. Please double-check submission structure.\")\n",
    "\n",
    "# === DEFINE SMAPE FUNCTION ===\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(\n",
    "        np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true)) / 2)\n",
    "    ) * 100\n",
    "\n",
    "# === LOAD TRAINED MODEL ===\n",
    "print(\"\\nüíæ Loading trained model...\")\n",
    "bundle = joblib.load(model_path)\n",
    "ridge = bundle[\"model\"]\n",
    "tfidf = bundle[\"vectorizer\"]\n",
    "scaler = bundle[\"scaler\"]\n",
    "print(\"‚úÖ Model loaded successfully.\")\n",
    "\n",
    "# === CLEAN TRAINING DATA ===\n",
    "print(\"\\nüßπ Cleaning train data for validation...\")\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].fillna(\"\").astype(str)\n",
    "train_df[\"price\"] = pd.to_numeric(train_df[\"price\"], errors=\"coerce\")\n",
    "train_df = train_df.dropna(subset=[\"price\"])\n",
    "train_df = train_df[train_df[\"price\"] > 0]\n",
    "\n",
    "# === FEATURE EXTRACTION FUNCTION ===\n",
    "def extract_features(text):\n",
    "    text = text.lower()\n",
    "    return {\n",
    "        \"len\": len(text),\n",
    "        \"num_words\": len(text.split()),\n",
    "        \"num_digits\": sum(c.isdigit() for c in text),\n",
    "        \"contains_pack\": int(\"pack\" in text),\n",
    "        \"contains_oz\": int(\"oz\" in text or \"ounce\" in text),\n",
    "        \"contains_lb\": int(\"lb\" in text),\n",
    "        \"contains_ml\": int(\"ml\" in text),\n",
    "        \"contains_kg\": int(\"kg\" in text),\n",
    "    }\n",
    "\n",
    "# === CREATE META FEATURES ===\n",
    "meta_features = pd.DataFrame([extract_features(t) for t in tqdm(train_df[\"catalog_content\"], desc=\"Extracting meta\")])\n",
    "X_meta = scaler.transform(meta_features)\n",
    "X_text = tfidf.transform(tqdm(train_df[\"catalog_content\"], desc=\"TF-IDF transforming\"))\n",
    "X = hstack([X_text, X_meta])\n",
    "y = np.log1p(train_df[\"price\"])\n",
    "\n",
    "# === VALIDATION SPLIT (same as Step 3) ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# === VALIDATION PREDICTIONS ===\n",
    "print(\"\\nü§ñ Predicting on validation split...\")\n",
    "y_pred_log = ridge.predict(X_valid)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_valid_exp = np.expm1(y_valid)\n",
    "\n",
    "# === METRICS ===\n",
    "mae = mean_absolute_error(y_valid_exp, y_pred)\n",
    "smape_score = smape(y_valid_exp, y_pred)\n",
    "\n",
    "print(f\"\\nüìä Local Validation Results:\")\n",
    "print(f\"‚úÖ MAE: {mae:.3f}\")\n",
    "print(f\"‚úÖ SMAPE: {smape_score:.2f}%\")\n",
    "\n",
    "# === FINAL CONFIRMATION ===\n",
    "print(f\"\\nüì¶ Submission ready at: {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7822cad8-93da-4ce0-ac22-01a2f5bf9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test shape: (75000, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 56/75000 [00:19<09:40, 129.11it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# === Paths ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "download_folder = os.path.join(base_path, \"data\", \"images\", \"test\")\n",
    "\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# === Load CSV ===\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"‚úÖ Test shape: {test_df.shape}\")\n",
    "\n",
    "# === Function to download one image ===\n",
    "def download_image(row):\n",
    "    url = row[\"image_link\"]\n",
    "    filename = os.path.basename(url)\n",
    "    save_path = os.path.join(download_folder, filename)\n",
    "    if os.path.exists(save_path):\n",
    "        return None  # skip already downloaded\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return (url, str(e))\n",
    "\n",
    "# === Parallel download ===\n",
    "failed = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=64) as executor:\n",
    "    for result in tqdm(executor.map(download_image, test_df.to_dict(\"records\")), total=len(test_df)):\n",
    "        if result:\n",
    "            failed.append(result)\n",
    "\n",
    "# === Summary ===\n",
    "print(f\"\\n‚úÖ Download complete: {len(test_df) - len(failed)} succeeded, {len(failed)} failed.\")\n",
    "if failed:\n",
    "    failed_log = os.path.join(download_folder, \"failed_test_downloads.txt\")\n",
    "    pd.DataFrame(failed, columns=[\"url\", \"error\"]).to_csv(failed_log, index=False)\n",
    "    print(f\"‚ö†Ô∏è Failed URLs saved to: {failed_log}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6902058-aa11-4f7a-a9aa-d29afab2d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Starting test image download (72222 URLs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72222/72222 [53:36<00:00, 22.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Download complete!\n",
      "‚ùå Failed: 1\n",
      "‚ö†Ô∏è Failed URLs saved to failed_test_downloads.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "download_folder = os.path.join(base_path, \"data\", \"images\", \"test\")\n",
    "os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# === SETTINGS ===\n",
    "MAX_WORKERS = 32\n",
    "RETRY_LIMIT = 3\n",
    "TIMEOUT = 8  # seconds\n",
    "\n",
    "# === READ CSV ===\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "urls = test_df[\"image_link\"].dropna().unique().tolist()\n",
    "\n",
    "failed = []\n",
    "\n",
    "def download_image(url):\n",
    "    filename = url.split(\"/\")[-1].replace(\"+\", \"_\")\n",
    "    filepath = os.path.join(download_folder, filename)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        return True\n",
    "\n",
    "    for attempt in range(RETRY_LIMIT):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=TIMEOUT)\n",
    "            if r.status_code == 200:\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                return True\n",
    "            else:\n",
    "                time.sleep(0.2)\n",
    "        except Exception:\n",
    "            time.sleep(0.5)\n",
    "    failed.append(url)\n",
    "    return False\n",
    "\n",
    "\n",
    "print(f\"üì• Starting test image download ({len(urls)} URLs)...\")\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(download_image, url): url for url in urls}\n",
    "    for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Downloading\"):\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Download complete!\")\n",
    "print(f\"‚ùå Failed: {len(failed)}\")\n",
    "\n",
    "if failed:\n",
    "    with open(os.path.join(base_path, \"failed_test_downloads.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(failed))\n",
    "    print(f\"‚ö†Ô∏è Failed URLs saved to failed_test_downloads.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a10ad8-fb53-47e6-85d6-bd4e53e0d25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to download: 404 Client Error: Not Found for url: https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\"\n",
    "save_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\data\\images\\test\\813CjSgHj0S.jpg\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"‚úÖ Image downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to download:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718214c0-82a8-413c-b561-4f02a5a90814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading model and vectorizer...\n",
      "üìÇ Loading test data...\n",
      "‚úÖ Test shape: (75000, 3)\n",
      "üßπ Cleaning text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:11<00:00, 6378.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî† Vectorizing with trained TF-IDF...\n",
      "ü§ñ Predicting prices...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 30000 features, but Ridge is expecting 30008 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# === PREDICT ===\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mü§ñ Predicting prices...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m preds \u001b[38;5;241m=\u001b[39m ridge\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# === CLIP OUTLIERS ===\u001b[39;00m\n\u001b[0;32m     52\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(preds, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:306\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:285\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    283\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m], reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    286\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coef_\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 30000 features, but Ridge is expecting 30008 features as input."
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# STEP 5 (FIXED): USE TRAINED TF-IDF + RIDGE MODEL\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "test_csv_path = os.path.join(data_path, \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# === LOAD TRAINED MODEL + TFIDF ===\n",
    "print(\"üì¶ Loading model and vectorizer...\")\n",
    "bundle = joblib.load(model_path)\n",
    "ridge = bundle[\"model\"]\n",
    "vectorizer = bundle[\"vectorizer\"]\n",
    "\n",
    "# === LOAD TEST DATA ===\n",
    "print(\"üìÇ Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"‚úÖ Test shape: {test_df.shape}\")\n",
    "\n",
    "# === CLEAN TEXT FUNCTION ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# === CLEAN TEST TEXT ===\n",
    "print(\"üßπ Cleaning text...\")\n",
    "tqdm.pandas()\n",
    "test_df[\"clean_content\"] = test_df[\"catalog_content\"].progress_apply(clean_text)\n",
    "\n",
    "# === TRANSFORM USING TRAINED TFIDF (DO NOT REFIT) ===\n",
    "print(\"üî† Vectorizing with trained TF-IDF...\")\n",
    "X_test = vectorizer.transform(test_df[\"clean_content\"])\n",
    "\n",
    "# === PREDICT ===\n",
    "print(\"ü§ñ Predicting prices...\")\n",
    "preds = ridge.predict(X_test)\n",
    "\n",
    "# === CLIP OUTLIERS ===\n",
    "preds = np.clip(preds, 0, 1000)\n",
    "\n",
    "# === SAVE SUBMISSION ===\n",
    "print(\"üíæ Saving submission file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission saved successfully: {submission_path}\")\n",
    "print(f\"‚úÖ Rows in submission: {len(submission_df)} | Expected: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeb6d26a-7964-4ab7-b747-9c8a6caa4100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['model', 'vectorizer', 'scaler'])\n",
      "Vectorizer features: 30000\n",
      "Ridge expected features: 30008\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\"\n",
    "\n",
    "bundle = joblib.load(path)\n",
    "print(\"Keys:\", bundle.keys())\n",
    "print(\"Vectorizer features:\", len(bundle[\"vectorizer\"].get_feature_names_out()))\n",
    "print(\"Ridge expected features:\", bundle[\"model\"].n_features_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841dc6eb-faea-4406-b5bc-1846a7c57a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading train data...\n",
      "‚úÖ Train shape: (75000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:11<00:00, 6364.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Training Ridge...\n",
      "‚úÖ Validation MAE: 14.1602\n",
      "üíæ Fixed model saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n",
      "Vectorizer features: 30000\n",
      "Ridge expected features: 30000\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# FIXED: Re-train Ridge with existing TF-IDF vectorizer\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "train_csv_path = os.path.join(data_path, \"train.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "\n",
    "# === LOAD CURRENT PICKLE ===\n",
    "bundle = joblib.load(model_path)\n",
    "vectorizer = bundle[\"vectorizer\"]     # use the 30k-feature one\n",
    "scaler = bundle.get(\"scaler\", None)\n",
    "\n",
    "# === LOAD TRAIN DATA ===\n",
    "print(\"üìÇ Loading train data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "# === CLEAN TEXT ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "train_df[\"clean_content\"] = train_df[\"catalog_content\"].progress_apply(clean_text)\n",
    "\n",
    "# === VECTORIZE ===\n",
    "X = vectorizer.transform(train_df[\"clean_content\"])\n",
    "y = train_df[\"price\"].values\n",
    "\n",
    "# === SPLIT ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# === TRAIN RIDGE ===\n",
    "print(\"‚öôÔ∏è Training Ridge...\")\n",
    "ridge = Ridge(alpha=2.0, random_state=42, solver=\"auto\", max_iter=2000)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# === VALIDATE ===\n",
    "y_pred = ridge.predict(X_val)\n",
    "mae = np.mean(np.abs(y_pred - y_val))\n",
    "print(f\"‚úÖ Validation MAE: {mae:.4f}\")\n",
    "\n",
    "# === SAVE FIXED PICKLE ===\n",
    "fixed_bundle = {\"model\": ridge, \"vectorizer\": vectorizer, \"scaler\": scaler}\n",
    "joblib.dump(fixed_bundle, model_path)\n",
    "\n",
    "print(f\"üíæ Fixed model saved successfully: {model_path}\")\n",
    "print(f\"Vectorizer features: {len(vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Ridge expected features: {ridge.n_features_in_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1adc60-4822-4dc2-b282-a0aa228ceeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading model and vectorizer...\n",
      "üìÇ Loading test data...\n",
      "‚úÖ Test shape: (75000, 3)\n",
      "üßπ Cleaning text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:11<00:00, 6757.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî† Vectorizing with trained TF-IDF...\n",
      "ü§ñ Predicting prices...\n",
      "üíæ Saving submission file...\n",
      "\n",
      "‚úÖ Submission saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Rows in submission: 75000 | Expected: 75000\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# STEP 5 (FIXED): USE TRAINED TF-IDF + RIDGE MODEL\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "test_csv_path = os.path.join(data_path, \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# === LOAD TRAINED MODEL + TFIDF ===\n",
    "print(\"üì¶ Loading model and vectorizer...\")\n",
    "bundle = joblib.load(model_path)\n",
    "ridge = bundle[\"model\"]\n",
    "vectorizer = bundle[\"vectorizer\"]\n",
    "\n",
    "# === LOAD TEST DATA ===\n",
    "print(\"üìÇ Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "print(f\"‚úÖ Test shape: {test_df.shape}\")\n",
    "\n",
    "# === CLEAN TEXT FUNCTION ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# === CLEAN TEST TEXT ===\n",
    "print(\"üßπ Cleaning text...\")\n",
    "tqdm.pandas()\n",
    "test_df[\"clean_content\"] = test_df[\"catalog_content\"].progress_apply(clean_text)\n",
    "\n",
    "# === TRANSFORM USING TRAINED TFIDF (DO NOT REFIT) ===\n",
    "print(\"üî† Vectorizing with trained TF-IDF...\")\n",
    "X_test = vectorizer.transform(test_df[\"clean_content\"])\n",
    "\n",
    "# === PREDICT ===\n",
    "print(\"ü§ñ Predicting prices...\")\n",
    "preds = ridge.predict(X_test)\n",
    "\n",
    "# === CLIP OUTLIERS ===\n",
    "preds = np.clip(preds, 0, 1000)\n",
    "\n",
    "# === SAVE SUBMISSION ===\n",
    "print(\"üíæ Saving submission file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission saved successfully: {submission_path}\")\n",
    "print(f\"‚úÖ Rows in submission: {len(submission_df)} | Expected: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a87fcf6-164e-43a4-b59c-222b2096c30e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmobilenet_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MobileNetV2, preprocess_input\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# STEP 6: MULTIMODAL (TEXT + IMAGE) MODEL ‚Äì Optimized Version\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "train_csv = os.path.join(data_path, \"train.csv\")\n",
    "train_images = os.path.join(data_path, \"images\", \"train\")\n",
    "embedding_path = os.path.join(base_path, \"train_image_embeddings.npy\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "\n",
    "# === LOAD TRAIN DATA ===\n",
    "train_df = pd.read_csv(train_csv)\n",
    "print(f\"üìÇ Loaded train data: {train_df.shape}\")\n",
    "\n",
    "# === LOAD TF-IDF VECTOR ===\n",
    "bundle = joblib.load(model_path)\n",
    "vectorizer = bundle[\"vectorizer\"]\n",
    "print(f\"‚úÖ TF-IDF vectorizer loaded with {len(vectorizer.get_feature_names_out())} features\")\n",
    "\n",
    "# === CLEAN TEXT ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9 ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "train_df[\"clean_content\"] = train_df[\"catalog_content\"].progress_apply(clean_text)\n",
    "\n",
    "# === TF-IDF FEATURES ===\n",
    "X_text = vectorizer.transform(train_df[\"clean_content\"])\n",
    "print(f\"‚úÖ TF-IDF features shape: {X_text.shape}\")\n",
    "\n",
    "# === IMAGE EMBEDDINGS ===\n",
    "if not os.path.exists(embedding_path):\n",
    "    print(\"üß† Loading MobileNetV2...\")\n",
    "    base_model = MobileNetV2(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "    model_img = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "    def get_embedding(img_path):\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "            arr = img_to_array(img)\n",
    "            arr = np.expand_dims(arr, axis=0)\n",
    "            arr = preprocess_input(arr)\n",
    "            feat = model_img.predict(arr, verbose=0)\n",
    "            return feat.flatten()\n",
    "        except Exception:\n",
    "            return np.zeros(1280)\n",
    "\n",
    "    print(\"üñºÔ∏è Extracting image embeddings...\")\n",
    "    embeddings = []\n",
    "    for link in tqdm(train_df[\"image_link\"], total=len(train_df)):\n",
    "        img_name = os.path.basename(link)\n",
    "        img_path = os.path.join(train_images, img_name)\n",
    "        embeddings.append(get_embedding(img_path))\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    np.save(embedding_path, embeddings)\n",
    "    print(f\"üíæ Saved image embeddings: {embedding_path}\")\n",
    "else:\n",
    "    print(\"‚úÖ Using pre-saved embeddings...\")\n",
    "    embeddings = np.load(embedding_path)\n",
    "\n",
    "print(f\"‚úÖ Image embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# === COMBINE TEXT + IMAGE ===\n",
    "X_combined = hstack([X_text, csr_matrix(embeddings)])\n",
    "y = train_df[\"price\"].values\n",
    "\n",
    "# === SPLIT ===\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# === TRAIN RIDGE ===\n",
    "print(\"‚öôÔ∏è Training Ridge Regression on combined features...\")\n",
    "ridge = Ridge(alpha=2.0, random_state=42, solver=\"auto\", max_iter=2000)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# === VALIDATE ===\n",
    "y_pred = ridge.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "smape = np.mean(2 * np.abs(y_pred - y_val) / (np.abs(y_pred) + np.abs(y_val))) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Validation MAE: {mae:.3f}\")\n",
    "print(f\"‚úÖ Validation SMAPE: {smape:.2f}%\")\n",
    "\n",
    "# === SAVE FINAL MULTIMODAL MODEL ===\n",
    "bundle_multi = {\"model\": ridge, \"vectorizer\": vectorizer, \"image_dim\": embeddings.shape[1]}\n",
    "joblib.dump(bundle_multi, os.path.join(base_path, \"tfidf_ridge_multimodal.pkl\"))\n",
    "print(f\"üíæ Multimodal model saved: tfidf_ridge_multimodal.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc5ba659-bd21-4400-87b8-bc9fbcda954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.16.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083873c9-de54-4437-9e62-2bf689bbc727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Train shape: (75000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75000/75000 [00:12<00:00, 6053.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Extracting simple image stats...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                     | 5772/75000 [16:09<3:13:49,  5.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(link)\n\u001b[0;32m     55\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(train_images, fname)\n\u001b[1;32m---> 56\u001b[0m     image_feats\u001b[38;5;241m.\u001b[39mappend(quick_image_stats(path))\n\u001b[0;32m     58\u001b[0m image_feats \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image_feats)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Image feature shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_feats\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mquick_image_stats\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquick_image_stats\u001b[39m(path):\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m img:  \u001b[38;5;66;03m# grayscale\u001b[39;00m\n\u001b[0;32m     43\u001b[0m             arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m     44\u001b[0m             mean_brightness \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(arr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# STEP 6 (FAST VERSION): Text + Lightweight Image Stats\n",
    "# ==========================================================\n",
    "\n",
    "import os, re, numpy as np, pandas as pd, joblib\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# === PATHS ===\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "train_csv = os.path.join(data_path, \"train.csv\")\n",
    "train_images = os.path.join(data_path, \"images\", \"train\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "\n",
    "# === LOAD DATA ===\n",
    "train_df = pd.read_csv(train_csv)\n",
    "print(f\"üìÇ Train shape: {train_df.shape}\")\n",
    "\n",
    "# === LOAD EXISTING TF-IDF ===\n",
    "bundle = joblib.load(model_path)\n",
    "vectorizer = bundle[\"vectorizer\"]\n",
    "\n",
    "# === CLEAN TEXT ===\n",
    "def clean_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9 ]\", \" \", t)\n",
    "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "tqdm.pandas()\n",
    "train_df[\"clean_content\"] = train_df[\"catalog_content\"].progress_apply(clean_text)\n",
    "X_text = vectorizer.transform(train_df[\"clean_content\"])\n",
    "\n",
    "# === LIGHTWEIGHT IMAGE FEATURES ===\n",
    "def quick_image_stats(path):\n",
    "    try:\n",
    "        with Image.open(path).convert(\"L\") as img:  # grayscale\n",
    "            arr = np.array(img)\n",
    "            mean_brightness = np.mean(arr)\n",
    "            std_brightness = np.std(arr)\n",
    "            w, h = img.size\n",
    "            return [mean_brightness, std_brightness, w, h]\n",
    "    except Exception:\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "print(\"üñºÔ∏è Extracting simple image stats...\")\n",
    "image_feats = []\n",
    "for link in tqdm(train_df[\"image_link\"], total=len(train_df)):\n",
    "    fname = os.path.basename(link)\n",
    "    path = os.path.join(train_images, fname)\n",
    "    image_feats.append(quick_image_stats(path))\n",
    "\n",
    "image_feats = np.array(image_feats)\n",
    "print(f\"‚úÖ Image feature shape: {image_feats.shape}\")\n",
    "\n",
    "# === COMBINE TEXT + IMAGE ===\n",
    "X_combined = hstack([X_text, csr_matrix(image_feats)])\n",
    "y = train_df[\"price\"].values\n",
    "\n",
    "# === TRAIN / VALIDATE ===\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_combined, y, test_size=0.15, random_state=42)\n",
    "ridge = Ridge(alpha=2.0, random_state=42)\n",
    "ridge.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred = ridge.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "smape = np.mean(2 * np.abs(y_pred - y_val) / (np.abs(y_pred) + np.abs(y_val))) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Validation MAE: {mae:.3f}\")\n",
    "print(f\"‚úÖ Validation SMAPE: {smape:.2f}%\")\n",
    "\n",
    "# === SAVE MODEL ===\n",
    "final_bundle = {\"model\": ridge, \"vectorizer\": vectorizer}\n",
    "joblib.dump(final_bundle, os.path.join(base_path, \"tfidf_ridge_lightimg.pkl\"))\n",
    "print(f\"üíæ Saved lightweight multimodal model: tfidf_ridge_lightimg.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aaa4329-7ecf-4b48-b5b5-8cc809c4f864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "\n",
      "üßπ Cleaning catalog text...\n",
      "‚úÖ Text cleaned!\n",
      "\n",
      "üìä Train size: 63750 | Validation size: 11250\n",
      "\n",
      "üî† Vectorizing text...\n",
      "‚úÖ TF-IDF features: 30000\n",
      "\n",
      "‚öôÔ∏è Training Ridge Regression...\n",
      "\n",
      "üìà Evaluating model...\n",
      "‚úÖ Validation MAE: 14.2110\n",
      "‚úÖ Validation SMAPE: 68.67%\n",
      "\n",
      "üíæ Model saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n",
      "\n",
      "üìÇ Loading test data...\n",
      "üî† Transforming test text...\n",
      "ü§ñ Predicting prices...\n",
      "\n",
      "‚úÖ Submission saved successfully: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Rows in submission: 75000 | Expected: 75000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025\n",
    "#  Model: TF-IDF + Ridge Regression\n",
    "#  Author: Md Irfan Raj\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD DATA\n",
    "# =============================================================\n",
    "\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "# Clean text\n",
    "print(\"\\nüßπ Cleaning catalog text...\")\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "print(\"‚úÖ Text cleaned!\")\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT DATA\n",
    "# =============================================================\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"],\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"\\nüìä Train size: {len(X_train)} | Validation size: {len(X_valid)}\")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF VECTORIZATION\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüî† Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1, 3),\n",
    "    sublinear_tf=True,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer.transform(X_valid)\n",
    "print(f\"‚úÖ TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "\n",
    "# =============================================================\n",
    "#  SCALING TARGET\n",
    "# =============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "\n",
    "# =============================================================\n",
    "#  TRAIN RIDGE MODEL\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Training Ridge Regression...\")\n",
    "ridge = Ridge(alpha=2.0, random_state=42, solver=\"auto\", max_iter=2000)\n",
    "ridge.fit(X_train_tfidf, y_train_scaled)\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüìà Evaluating model...\")\n",
    "y_pred_valid_scaled = ridge.predict(X_valid_tfidf)\n",
    "y_pred_valid = scaler.inverse_transform(y_pred_valid_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "smape_score = smape(y_valid, y_pred_valid)\n",
    "\n",
    "print(f\"‚úÖ Validation MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ Validation SMAPE: {smape_score:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL\n",
    "# =============================================================\n",
    "\n",
    "joblib.dump(\n",
    "    {\"model\": ridge, \"vectorizer\": vectorizer, \"scaler\": scaler},\n",
    "    model_path\n",
    ")\n",
    "print(f\"\\nüíæ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST.CSV\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüìÇ Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# Reload model safely\n",
    "bundle = joblib.load(model_path)\n",
    "ridge = bundle[\"model\"]\n",
    "vectorizer = bundle[\"vectorizer\"]\n",
    "scaler = bundle[\"scaler\"]\n",
    "\n",
    "# Transform test\n",
    "print(\"üî† Transforming test text...\")\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "\n",
    "# Predict\n",
    "print(\"ü§ñ Predicting prices...\")\n",
    "preds_scaled = ridge.predict(X_test_tfidf)\n",
    "preds = scaler.inverse_transform(preds_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Clip unrealistic predictions\n",
    "preds = np.clip(preds, 0, 1000)\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE SUBMISSION\n",
    "# =============================================================\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission saved successfully: {submission_path}\")\n",
    "print(f\"‚úÖ Rows in submission: {len(submission)} | Expected: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22270f25-5faf-4bf8-b66f-81cd3d2e0dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "\n",
      "üßπ Cleaning...\n",
      "‚úÖ Text cleaned.\n",
      "\n",
      "üìä Train: 63750, Validation: 11250\n",
      "\n",
      "üî† Vectorizing...\n",
      "‚úÖ TF-IDF features: 50000\n",
      "\n",
      "‚öôÔ∏è Training Ridge...\n",
      "\n",
      "üìà Evaluating...\n",
      "‚úÖ MAE: 12.3521\n",
      "‚úÖ SMAPE: 54.15%\n",
      "\n",
      "üíæ Saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n",
      "\n",
      "üìÇ Loading test data...\n",
      "\n",
      "‚úÖ Saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv | Rows: 75000 = Expected: 75000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (Optimized)\n",
    "#  TF-IDF + Ridge Regression (Best for fast SMAPE drop)\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "# Load training data\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "print(\"\\nüßπ Cleaning...\")\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "print(\"‚úÖ Text cleaned.\")\n",
    "\n",
    "# Split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"], train_df[\"price\"],\n",
    "    test_size=0.15, random_state=42\n",
    ")\n",
    "print(f\"\\nüìä Train: {len(X_train)}, Validation: {len(X_valid)}\")\n",
    "\n",
    "# TF-IDF (improved)\n",
    "print(\"\\nüî† Vectorizing...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer.transform(X_valid)\n",
    "print(f\"‚úÖ TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "\n",
    "# Log-transform target\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# Ridge (lower alpha)\n",
    "print(\"\\n‚öôÔ∏è Training Ridge...\")\n",
    "ridge = Ridge(alpha=0.7, random_state=42, solver=\"auto\", max_iter=2000)\n",
    "ridge.fit(X_train_tfidf, y_train_log)\n",
    "\n",
    "# Validation\n",
    "print(\"\\nüìà Evaluating...\")\n",
    "y_pred_valid_log = ridge.predict(X_valid_tfidf)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "smape_score = smape(y_valid, y_pred_valid)\n",
    "print(f\"‚úÖ MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ SMAPE: {smape_score:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump({\"model\": ridge, \"vectorizer\": vectorizer}, model_path)\n",
    "print(f\"\\nüíæ Saved: {model_path}\")\n",
    "\n",
    "# Test inference\n",
    "print(\"\\nüìÇ Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "bundle = joblib.load(model_path)\n",
    "ridge = bundle[\"model\"]\n",
    "vectorizer = bundle[\"vectorizer\"]\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "preds_log = ridge.predict(X_test_tfidf)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 1000)\n",
    "\n",
    "# Submission\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved: {submission_path} | Rows: {len(submission)} = Expected: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e975de3a-0041-4cd3-90b7-1d631d1c2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "\n",
      "üßπ Cleaning catalog text...\n",
      "‚úÖ Text cleaned!\n",
      "\n",
      "üìä Train size: 63750, Validation size: 11250\n",
      "\n",
      "üî† Vectorizing text...\n",
      "‚úÖ TF-IDF features: 50000\n",
      "\n",
      "üî¨ GridSearching Ridge alpha ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "‚úÖ Best alpha found: 1.26\n",
      "\n",
      "üìà Evaluating best model...\n",
      "‚úÖ Validation MAE: 12.3353\n",
      "‚úÖ Validation SMAPE: 53.96%\n",
      "\n",
      "üíæ Model saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_model.pkl\n",
      "\n",
      "üìÇ Loading test data...\n",
      "ü§ñ Predicting prices...\n",
      "\n",
      "‚úÖ Submission saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Rows: 75000 (Expected: 75000)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (Optimized)\n",
    "#  Model: TF-IDF + Feature Engineering + Ridge Regression + GridSearch\n",
    "#  Author: Md Irfan Raj (with SMAPE Reduction)\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "def get_additional_features(df):\n",
    "    # Numeric features from product description\n",
    "    df = df.copy()\n",
    "    df[\"desc_word_count\"] = df[\"catalog_content\"].apply(lambda x: len(x.split()))\n",
    "    df[\"desc_char_count\"] = df[\"catalog_content\"].apply(len)\n",
    "    # Optionally, add more domain features here (category, rating) if available\n",
    "    return df[[\"desc_word_count\", \"desc_char_count\"]]\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD DATA\n",
    "# =============================================================\n",
    "\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "print(\"\\nüßπ Cleaning catalog text...\")\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "print(\"‚úÖ Text cleaned!\")\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT DATA\n",
    "# =============================================================\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"],\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"\\nüìä Train size: {len(X_train)}, Validation size: {len(X_valid)}\")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF VECTORIZATION\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüî† Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer.transform(X_valid)\n",
    "print(f\"‚úÖ TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "\n",
    "# =============================================================\n",
    "#  ADDITIONAL NUMERIC FEATURES\n",
    "# =============================================================\n",
    "\n",
    "feat_train = get_additional_features(pd.DataFrame({\"catalog_content\": X_train}))\n",
    "feat_valid = get_additional_features(pd.DataFrame({\"catalog_content\": X_valid}))\n",
    "scaler_feat = StandardScaler()\n",
    "feat_train_scaled = scaler_feat.fit_transform(feat_train)\n",
    "feat_valid_scaled = scaler_feat.transform(feat_valid)\n",
    "\n",
    "# Combine TF-IDF and numeric features\n",
    "X_train_final = sparse.hstack([X_train_tfidf, feat_train_scaled])\n",
    "X_valid_final = sparse.hstack([X_valid_tfidf, feat_valid_scaled])\n",
    "\n",
    "# =============================================================\n",
    "#  LOG TRANSFORM TARGET\n",
    "# =============================================================\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  GRID SEARCH RIDGE REGRESSION\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüî¨ GridSearching Ridge alpha ...\")\n",
    "alphas = np.arange(0.01, 3.0, 0.25)\n",
    "ridge = Ridge(max_iter=2000)\n",
    "params = {\"alpha\": alphas}\n",
    "gs = GridSearchCV(ridge, params, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1, verbose=1)\n",
    "gs.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"‚úÖ Best alpha found:\", gs.best_params_[\"alpha\"])\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüìà Evaluating best model...\")\n",
    "y_pred_valid_log = gs.predict(X_valid_final)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "smape_score = smape(y_valid, y_pred_valid)\n",
    "\n",
    "print(f\"‚úÖ Validation MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ Validation SMAPE: {smape_score:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL & OBJECTS\n",
    "# =============================================================\n",
    "\n",
    "joblib.dump({\n",
    "    \"model\": gs,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler_feat\": scaler_feat\n",
    "}, model_path)\n",
    "print(f\"\\nüíæ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST.CSV\n",
    "# =============================================================\n",
    "\n",
    "print(\"\\nüìÇ Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "feat_test = get_additional_features(test_df)\n",
    "feat_test_scaled = scaler_feat.transform(feat_test)\n",
    "X_test_final = sparse.hstack([X_test_tfidf, feat_test_scaled])\n",
    "\n",
    "print(\"ü§ñ Predicting prices...\")\n",
    "preds_log = gs.predict(X_test_final)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 1000)\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE SUBMISSION\n",
    "# =============================================================\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: {submission_path}\")\n",
    "print(f\"‚úÖ Rows: {len(submission)} (Expected: {len(test_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0109a697-fc63-4297-b4b8-5f25afadbce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "‚úÖ Removed outliers > $145.25\n",
      "üî† Vectorizing text...\n",
      "‚öôÔ∏è GridSearching Ridge...\n",
      "‚úÖ Best alpha: 1.5000000000000004\n",
      "‚úÖ MAE: 10.5558\n",
      "‚úÖ SMAPE (0‚Äì200): 52.57%\n",
      "‚úÖ If 0‚Äì100 scale: 26.28%\n",
      "\n",
      "üíæ Model saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_final.pkl\n",
      "üîç Loading test data...\n",
      "\n",
      "‚úÖ Submission saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (FINAL OPTIMIZED)\n",
    "#  Model: TF-IDF + Features + Log + Outlier Capping\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_final.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    # Standard SMAPE: 0‚Äì200%, but competition likely uses 0‚Äì100% version\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 200  # returns 0‚Äì200; divide by 2 if needed\n",
    "\n",
    "def get_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"word_count\"] = df[\"catalog_content\"].apply(lambda x: len(x.split()))\n",
    "    df[\"char_count\"] = df[\"catalog_content\"].apply(len)\n",
    "    df[\"has_digit\"] = df[\"catalog_content\"].apply(lambda x: 1 if any(c.isdigit() for c in x) else 0)\n",
    "    return df[[\"word_count\", \"char_count\", \"has_digit\"]]\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD & CLEAN DATA\n",
    "# =============================================================\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "# Remove extreme outliers (top 1% of prices)\n",
    "upper_quantile = train_df[\"price\"].quantile(0.99)\n",
    "train_df = train_df[train_df[\"price\"] <= upper_quantile]\n",
    "print(f\"‚úÖ Removed outliers > ${upper_quantile:.2f}\")\n",
    "\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT DATA\n",
    "# =============================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"],\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF + FEATURE ENGINEERING\n",
    "# =============================================================\n",
    "print(\"üî† Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer.transform(X_valid)\n",
    "\n",
    "# Add engineered features\n",
    "feat_train = get_features(pd.DataFrame({\"catalog_content\": X_train}))\n",
    "feat_valid = get_features(pd.DataFrame({\"catalog_content\": X_valid}))\n",
    "scaler = StandardScaler()\n",
    "feat_train_scaled = scaler.fit_transform(feat_train)\n",
    "feat_valid_scaled = scaler.transform(feat_valid)\n",
    "\n",
    "X_train_final = sparse.hstack([X_train_tfidf, feat_train_scaled])\n",
    "X_valid_final = sparse.hstack([X_valid_tfidf, feat_valid_scaled])\n",
    "\n",
    "# =============================================================\n",
    "#  LOG TRANSFORM + MODEL TRAINING\n",
    "# =============================================================\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "print(\"‚öôÔ∏è GridSearching Ridge...\")\n",
    "ridge = Ridge(max_iter=2000)\n",
    "params = {\"alpha\": np.arange(0.1, 2.0, 0.2)}\n",
    "gs = GridSearchCV(ridge, params, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "gs.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"‚úÖ Best alpha:\", gs.best_params_[\"alpha\"])\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "y_pred_valid_log = gs.predict(X_valid_final)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "y_pred_valid = np.clip(y_pred_valid, 0, 1000)\n",
    "\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "smape_score = smape(y_valid, y_pred_valid)  # 0‚Äì200 scale\n",
    "print(f\"‚úÖ MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ SMAPE (0‚Äì200): {smape_score:.2f}%\")\n",
    "print(f\"‚úÖ If 0‚Äì100 scale: {smape_score / 2:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL\n",
    "# =============================================================\n",
    "joblib.dump({\n",
    "    \"model\": gs,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler\": scaler,\n",
    "    \"upper_quantile\": upper_quantile\n",
    "}, model_path)\n",
    "print(f\"\\nüíæ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST\n",
    "# =============================================================\n",
    "print(\"üîç Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "feat_test = get_features(test_df)\n",
    "feat_test_scaled = scaler.transform(feat_test)\n",
    "X_test_final = sparse.hstack([X_test_tfidf, feat_test_scaled])\n",
    "\n",
    "preds_log = gs.predict(X_test_final)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 1000)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e14f5b9-bebf-41f8-a3e3-acbc1b36dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "‚úÖ Training data after capping at 75,000: 75000 rows\n",
      "‚úÖ Winsorized prices at $145.25\n",
      "üî† Vectorizing text...\n",
      "‚öôÔ∏è GridSearching Ridge...\n",
      "‚úÖ Best alpha: 2.0\n",
      "‚úÖ MAE: 11.6155\n",
      "‚úÖ SMAPE (0‚Äì100): 26.94%\n",
      "\n",
      "üíæ Model saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\tfidf_ridge_final.pkl\n",
      "üîç Loading test data...\n",
      "\n",
      "‚úÖ Submission saved: C:\\Users\\mdirf\\Desktop\\amazon\\My\\submission.csv\n",
      "‚úÖ Max predicted price: 253.31\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (FINAL)\n",
    "#  Target: SMAPE ‚â§ 30% (0‚Äì100 scale) | Max Price: 75,000\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"tfidf_ridge_final.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape_0_100(y_true, y_pred):\n",
    "    # Returns SMAPE in 0‚Äì100% range\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100  # 0‚Äì100%\n",
    "\n",
    "def get_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"word_count\"] = df[\"catalog_content\"].apply(lambda x: len(x.split()))\n",
    "    df[\"char_count\"] = df[\"catalog_content\"].apply(len)\n",
    "    df[\"digit_count\"] = df[\"catalog_content\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df[\"has_price\"] = df[\"catalog_content\"].apply(lambda x: 1 if re.search(r'\\\\d{3,}', x) else 0)\n",
    "    df[\"desc_len_bin\"] = pd.cut(df[\"char_count\"], bins=5, labels=False)\n",
    "    return df[[\"word_count\", \"char_count\", \"digit_count\", \"has_price\", \"desc_len_bin\"]]\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD DATA\n",
    "# =============================================================\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "# Cap prices at 75,000 (do not clip during training)\n",
    "train_df = train_df[train_df[\"price\"] <= 75000].copy()\n",
    "print(f\"‚úÖ Training data after capping at 75,000: {len(train_df)} rows\")\n",
    "\n",
    "# Winsorize price (cap at 99th percentile)\n",
    "p99 = train_df[\"price\"].quantile(0.99)\n",
    "train_df[\"price\"] = np.clip(train_df[\"price\"], None, p99)\n",
    "print(f\"‚úÖ Winsorized prices at ${p99:.2f}\")\n",
    "\n",
    "# Clean text\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT\n",
    "# =============================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"],\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF\n",
    "# =============================================================\n",
    "print(\"üî† Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=60000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=3,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer.transform(X_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  FEATURE ENGINEERING\n",
    "# =============================================================\n",
    "feat_train = get_features(pd.DataFrame({\"catalog_content\": X_train}))\n",
    "feat_valid = get_features(pd.DataFrame({\"catalog_content\": X_valid}))\n",
    "scaler = StandardScaler()\n",
    "feat_train_scaled = scaler.fit_transform(feat_train)\n",
    "feat_valid_scaled = scaler.transform(feat_valid)\n",
    "\n",
    "X_train_final = sparse.hstack([X_train_tfidf, feat_train_scaled])\n",
    "X_valid_final = sparse.hstack([X_valid_tfidf, feat_valid_scaled])\n",
    "\n",
    "# =============================================================\n",
    "#  LOG TRANSFORM\n",
    "# =============================================================\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  GRID SEARCH RIDGE\n",
    "# =============================================================\n",
    "print(\"‚öôÔ∏è GridSearching Ridge...\")\n",
    "ridge = Ridge(max_iter=3000)\n",
    "params = {\"alpha\": [0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "gs = GridSearchCV(ridge, params, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "gs.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"‚úÖ Best alpha:\", gs.best_params_[\"alpha\"])\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "y_pred_valid_log = gs.predict(X_valid_final)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "y_pred_valid = np.clip(y_pred_valid, 0, 75000)  # Clip at 75,000\n",
    "\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "smape_score = smape_0_100(y_valid, y_pred_valid)  # Now 0‚Äì100%\n",
    "print(f\"‚úÖ MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ SMAPE (0‚Äì100): {smape_score:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL\n",
    "# =============================================================\n",
    "joblib.dump({\n",
    "    \"model\": gs,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler\": scaler,\n",
    "    \"p99\": p99\n",
    "}, model_path)\n",
    "print(f\"\\nüíæ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST\n",
    "# =============================================================\n",
    "print(\"üîç Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "feat_test = get_features(test_df)\n",
    "feat_test_scaled = scaler.transform(feat_test)\n",
    "X_test_final = sparse.hstack([X_test_tfidf, feat_test_scaled])\n",
    "\n",
    "preds_log = gs.predict(X_test_final)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 75000)  # Final clip at 75,000\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: {submission_path}\")\n",
    "print(f\"‚úÖ Max predicted price: {preds.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d84d248-d7d4-4438-8aa6-c90854c55676",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#  PATHS\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[0;32m     21\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmdirf\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mamazon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (ADVANCED)\n",
    "#  Model: LightGBM + TF-IDF + Feature Engineering\n",
    "#  Target: SMAPE ‚â§ 30% (0‚Äì100 scale) | Max Price: 75,000\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"lgbm_price_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape_0_100(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "def get_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"word_count\"] = df[\"catalog_content\"].apply(lambda x: len(x.split()))\n",
    "    df[\"char_count\"] = df[\"catalog_content\"].apply(len)\n",
    "    df[\"digit_count\"] = df[\"catalog_content\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df[\"has_price\"] = df[\"catalog_content\"].apply(lambda x: 1 if re.search(r'\\d{3,}', x) else 0)\n",
    "    df[\"desc_len_bin\"] = pd.cut(df[\"char_count\"], bins=5, labels=False).astype(int)\n",
    "    df[\"price_hint\"] = df[\"catalog_content\"].apply(lambda x: 1 if any(w in x for w in ['rs', 'price', 'only', 'buy', 'deal']) else 0)\n",
    "    return df[[\"word_count\", \"char_count\", \"digit_count\", \"has_price\", \"desc_len_bin\", \"price_hint\"]]\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD DATA\n",
    "# =============================================================\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "# Cap price at 75,000\n",
    "train_df = train_df[train_df[\"price\"] <= 75000].copy()\n",
    "print(f\"‚úÖ After capping at 75,000: {len(train_df)} rows\")\n",
    "\n",
    "# Winsorize top 1%\n",
    "p99 = train_df[\"price\"].quantile(0.99)\n",
    "train_df[\"price\"] = np.clip(train_df[\"price\"], None, p99)\n",
    "print(f\"‚úÖ Winsorized at ${p99:.2f}\")\n",
    "\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT\n",
    "# =============================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop(columns=[\"price\"]),\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF\n",
    "# =============================================================\n",
    "print(\"üî† Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=60000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train[\"catalog_content\"])\n",
    "X_valid_tfidf = vectorizer.transform(X_valid[\"catalog_content\"])\n",
    "\n",
    "# =============================================================\n",
    "#  FEATURE ENGINEERING\n",
    "# =============================================================\n",
    "feat_train = get_features(X_train)\n",
    "feat_valid = get_features(X_valid)\n",
    "scaler = StandardScaler()\n",
    "feat_train_scaled = scaler.fit_transform(feat_train)\n",
    "feat_valid_scaled = scaler.transform(feat_valid)\n",
    "\n",
    "# Combine TF-IDF and features\n",
    "X_train_final = sparse.hstack([X_train_tfidf, feat_train_scaled])\n",
    "X_valid_final = sparse.hstack([X_valid_tfidf, feat_valid_scaled])\n",
    "\n",
    "# =============================================================\n",
    "#  LOG TRANSFORM\n",
    "# =============================================================\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  LIGHTGBM MODEL\n",
    "# =============================================================\n",
    "print(\"üöÄ Training LightGBM...\")\n",
    "lgb_train = lgb.Dataset(X_train_final, label=y_train_log)\n",
    "lgb_valid = lgb.Dataset(X_valid_final, label=y_valid_log, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "y_pred_valid_log = model.predict(X_valid_final, num_iteration=model.best_iteration)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "y_pred_valid = np.clip(y_pred_valid, 0, 75000)\n",
    "\n",
    "mae = mean_absolute_error(y_valid, y_pred_valid)\n",
    "smape_score = smape_0_100(y_valid, y_pred_valid)\n",
    "print(f\"‚úÖ MAE: {mae:.4f}\")\n",
    "print(f\"‚úÖ SMAPE (0‚Äì100): {smape_score:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL\n",
    "# =============================================================\n",
    "joblib.dump({\n",
    "    \"model\": model,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler\": scaler,\n",
    "    \"best_iter\": model.best_iteration\n",
    "}, model_path)\n",
    "print(f\"\\nüíæ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST\n",
    "# =============================================================\n",
    "print(\"üîç Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "feat_test = get_features(test_df)\n",
    "feat_test_scaled = scaler.transform(feat_test)\n",
    "X_test_final = sparse.hstack([X_test_tfidf, feat_test_scaled])\n",
    "\n",
    "preds_log = model.predict(X_test_final, num_iteration=model.best_iteration)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 75000)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: {submission_path}\")\n",
    "print(f\"‚úÖ Max predicted price: {preds.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7f7d18-a766-4261-8227-6b31e9e3117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from catboost) (3.9.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from catboost) (1.13.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\mdirf\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.3)\n",
      "Downloading catboost-1.2.8-cp312-cp312-win_amd64.whl (102.4 MB)\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.8/102.4 MB 9.1 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 4.2/102.4 MB 13.9 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 5.5/102.4 MB 9.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 6.0/102.4 MB 7.5 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 7.3/102.4 MB 7.5 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 8.7/102.4 MB 6.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 8.9/102.4 MB 6.5 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 10.2/102.4 MB 6.2 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 12.1/102.4 MB 6.4 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 12.6/102.4 MB 6.3 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 13.9/102.4 MB 6.0 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 14.2/102.4 MB 6.1 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 15.7/102.4 MB 6.0 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 16.8/102.4 MB 5.8 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 17.3/102.4 MB 5.7 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 17.8/102.4 MB 5.4 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 18.1/102.4 MB 5.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 18.4/102.4 MB 5.1 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 18.9/102.4 MB 4.8 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 19.1/102.4 MB 4.7 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 19.7/102.4 MB 4.5 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 19.9/102.4 MB 4.4 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 20.2/102.4 MB 4.3 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 20.7/102.4 MB 4.2 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 21.0/102.4 MB 4.1 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 21.5/102.4 MB 4.0 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 22.0/102.4 MB 3.9 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 22.3/102.4 MB 3.9 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 22.8/102.4 MB 3.8 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 23.1/102.4 MB 3.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 23.6/102.4 MB 3.7 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 23.9/102.4 MB 3.6 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 24.4/102.4 MB 3.6 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 24.9/102.4 MB 3.5 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 25.2/102.4 MB 3.5 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 25.7/102.4 MB 3.4 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 26.0/102.4 MB 3.4 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 26.5/102.4 MB 3.4 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 27.0/102.4 MB 3.3 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 27.5/102.4 MB 3.3 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 27.8/102.4 MB 3.3 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 28.3/102.4 MB 3.3 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 28.8/102.4 MB 3.2 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 29.4/102.4 MB 3.2 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 29.9/102.4 MB 3.2 MB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 30.4/102.4 MB 3.2 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 30.9/102.4 MB 3.2 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 31.5/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 32.0/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 32.5/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 33.0/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 33.6/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 34.1/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 34.6/102.4 MB 3.1 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 35.1/102.4 MB 3.1 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 35.7/102.4 MB 3.1 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 36.4/102.4 MB 3.1 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 37.0/102.4 MB 3.1 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 37.7/102.4 MB 3.1 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 38.3/102.4 MB 3.1 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 39.1/102.4 MB 3.1 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 39.8/102.4 MB 3.1 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 40.6/102.4 MB 3.1 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 41.4/102.4 MB 3.1 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 42.2/102.4 MB 3.1 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 43.0/102.4 MB 3.1 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 44.0/102.4 MB 3.1 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 45.1/102.4 MB 3.2 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 46.1/102.4 MB 3.2 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 47.2/102.4 MB 3.2 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 48.5/102.4 MB 3.3 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 49.8/102.4 MB 3.3 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 50.9/102.4 MB 3.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 52.2/102.4 MB 3.4 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 53.7/102.4 MB 3.4 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 55.1/102.4 MB 3.5 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 56.1/102.4 MB 3.5 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 57.4/102.4 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 57.9/102.4 MB 3.5 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 58.7/102.4 MB 3.5 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 59.2/102.4 MB 3.5 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 59.8/102.4 MB 3.5 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 60.3/102.4 MB 3.5 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 61.1/102.4 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 61.6/102.4 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 62.1/102.4 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 62.9/102.4 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 63.7/102.4 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 64.2/102.4 MB 3.5 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 65.0/102.4 MB 3.4 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 65.5/102.4 MB 3.4 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 66.3/102.4 MB 3.4 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 66.8/102.4 MB 3.4 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 67.6/102.4 MB 3.4 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 68.4/102.4 MB 3.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 68.9/102.4 MB 3.4 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 69.7/102.4 MB 3.4 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 70.3/102.4 MB 3.4 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 71.0/102.4 MB 3.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 71.8/102.4 MB 3.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 72.4/102.4 MB 3.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 73.1/102.4 MB 3.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 73.7/102.4 MB 3.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 74.4/102.4 MB 3.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 75.2/102.4 MB 3.4 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 75.8/102.4 MB 3.4 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 76.5/102.4 MB 3.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 77.3/102.4 MB 3.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 78.1/102.4 MB 3.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 78.9/102.4 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 79.4/102.4 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 80.2/102.4 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 81.0/102.4 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 81.8/102.4 MB 3.4 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 82.6/102.4 MB 3.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 83.4/102.4 MB 3.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 84.1/102.4 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 84.9/102.4 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 85.7/102.4 MB 3.4 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 86.8/102.4 MB 3.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 87.6/102.4 MB 3.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 88.3/102.4 MB 3.4 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 89.1/102.4 MB 3.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 89.9/102.4 MB 3.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 91.0/102.4 MB 3.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 92.0/102.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 92.8/102.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 93.8/102.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 94.9/102.4 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 95.9/102.4 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 97.0/102.4 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 98.0/102.4 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 99.4/102.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  100.4/102.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  101.7/102.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  102.2/102.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 102.4/102.4 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.8 graphviz-0.21\n"
     ]
    }
   ],
   "source": [
    "pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13c1d4c-2868-41e1-8a7c-a4aac79dc731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ Train shape: (75000, 4)\n",
      "‚úÖ After capping at 75,000: 75000 rows\n",
      "‚úÖ Winsorized prices at $145.25\n",
      "üî† Vectorizing text...\n",
      "üìä Engineering numeric features...\n",
      "üöÄ Training CatBoost Regressor...\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 137\u001b[0m\n\u001b[0;32m    126\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRegressor(\n\u001b[0;32m    127\u001b[0m     iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m    128\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m    134\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Train with evaluation\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    138\u001b[0m     X_train_final, y_train_log,\n\u001b[0;32m    139\u001b[0m     eval_set\u001b[38;5;241m=\u001b[39m(X_valid_final, y_valid_log),\n\u001b[0;32m    140\u001b[0m     use_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m#  VALIDATION\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìà Validating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, graph, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline,\n\u001b[0;32m   5874\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5875\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5876\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   2411\u001b[0m         train_pool,\n\u001b[0;32m   2412\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2413\u001b[0m         params,\n\u001b[0;32m   2414\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2415\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2416\u001b[0m     )\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5023\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5072\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (FINAL)\n",
    "#  Model: CatBoost + TF-IDF + Features | Target: SMAPE ‚â§ 30%\n",
    "#  Output: Clipped at 75,000 | SMAPE in 0‚Äì100% scale\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"catboost_price_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean catalog text\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape_0_100(y_true, y_pred):\n",
    "    \"\"\"SMAPE on 0‚Äì100% scale\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\"Engineer numeric features from text\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"word_count\"] = df[\"catalog_content\"].apply(lambda x: len(x.split()))\n",
    "    df[\"char_count\"] = df[\"catalog_content\"].apply(len)\n",
    "    df[\"digit_count\"] = df[\"catalog_content\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df[\"has_price\"] = df[\"catalog_content\"].apply(lambda x: 1 if re.search(r'\\d{3,}', x) else 0)\n",
    "    df[\"price_hint\"] = df[\"catalog_content\"].apply(\n",
    "        lambda x: 1 if any(w in x for w in ['rs', 'price', 'only', 'buy', 'deal', 'off', 'discount']) else 0\n",
    "    )\n",
    "    df[\"desc_len_bin\"] = pd.cut(df[\"char_count\"], bins=5, labels=False).astype(int)\n",
    "    return df[[\"word_count\", \"char_count\", \"digit_count\", \"has_price\", \"price_hint\", \"desc_len_bin\"]]\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD DATA\n",
    "# =============================================================\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(f\"‚úÖ Train shape: {train_df.shape}\")\n",
    "\n",
    "# Cap price at 75,000\n",
    "train_df = train_df[train_df[\"price\"] <= 75000].copy()\n",
    "print(f\"‚úÖ After capping at 75,000: {len(train_df)} rows\")\n",
    "\n",
    "# Winsorize top 1% to reduce outlier impact\n",
    "p99 = train_df[\"price\"].quantile(0.99)\n",
    "train_df[\"price\"] = np.clip(train_df[\"price\"], None, p99)\n",
    "print(f\"‚úÖ Winsorized prices at ${p99:.2f}\")\n",
    "\n",
    "# Clean text\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT\n",
    "# =============================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop(columns=[\"price\"]),\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF VECTORIZATION\n",
    "# =============================================================\n",
    "print(\"üî† Vectorizing text...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=60000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=2,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train[\"catalog_content\"])\n",
    "X_valid_tfidf = vectorizer.transform(X_valid[\"catalog_content\"])\n",
    "\n",
    "# =============================================================\n",
    "#  FEATURE ENGINEERING\n",
    "# =============================================================\n",
    "print(\"üìä Engineering numeric features...\")\n",
    "feat_train = get_features(X_train)\n",
    "feat_valid = get_features(X_valid)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "feat_train_scaled = scaler.fit_transform(feat_train)\n",
    "feat_valid_scaled = scaler.transform(feat_valid)\n",
    "\n",
    "# Combine TF-IDF and features\n",
    "X_train_final = sparse.hstack([X_train_tfidf, feat_train_scaled])\n",
    "X_valid_final = sparse.hstack([X_valid_tfidf, feat_valid_scaled])\n",
    "\n",
    "# Log-transform target for stability\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  CATBOOST MODEL\n",
    "# =============================================================\n",
    "print(\"üöÄ Training CatBoost Regressor...\")\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function='MAE',\n",
    "    verbose=100,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Train with evaluation\n",
    "model.fit(\n",
    "    X_train_final, y_train_log,\n",
    "    eval_set=(X_valid_final, y_valid_log),\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "print(\"üìà Validating model...\")\n",
    "y_pred_valid_log = model.predict(X_valid_final)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "y_pred_valid = np.clip(y_pred_valid, 0, 75000)  # Final clip\n",
    "\n",
    "smape_score = smape_0_100(y_valid, y_pred_valid)\n",
    "print(f\"‚úÖ SMAPE (0‚Äì100): {smape_score:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL\n",
    "# =============================================================\n",
    "print(\"üíæ Saving model...\")\n",
    "joblib.dump({\n",
    "    \"model\": model,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler\": scaler,\n",
    "    \"p99\": p99\n",
    "}, model_path)\n",
    "print(f\"‚úÖ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST DATA\n",
    "# =============================================================\n",
    "print(\"üîç Loading test data...\")\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# Transform test\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "feat_test = get_features(test_df)\n",
    "feat_test_scaled = scaler.transform(feat_test)\n",
    "X_test_final = sparse.hstack([X_test_tfidf, feat_test_scaled])\n",
    "\n",
    "# Predict\n",
    "print(\"ü§ñ Predicting prices...\")\n",
    "preds_log = model.predict(X_test_final)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 75000)  # Ensure max is 75,000\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE SUBMISSION\n",
    "# =============================================================\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: {submission_path}\")\n",
    "print(f\"‚úÖ Max predicted price: {preds.max():.2f}\")\n",
    "print(f\"‚úÖ Rows: {len(submission)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc116205-5adc-459a-b939-a7723880084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading training data...\n",
      "‚úÖ After capping at 75,000: 75000 rows\n",
      "‚úÖ Winsorized at $145.25\n",
      "üî† Vectorizing text (25k features)...\n",
      "‚öôÔ∏è GridSearching Ridge...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  SMART PRODUCT PRICING - ML CHALLENGE 2025 (MEMORY-SAFE)\n",
    "#  Model: Ridge + TF-IDF (25k) + Features | Target: SMAPE < 35%\n",
    "#  Output: Clipped at 75,000 | SMAPE in 0‚Äì100% scale\n",
    "# =============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =============================================================\n",
    "#  PATHS\n",
    "# =============================================================\n",
    "base_path = r\"C:\\Users\\mdirf\\Desktop\\amazon\\My\"\n",
    "train_csv_path = os.path.join(base_path, \"data\", \"train.csv\")\n",
    "test_csv_path = os.path.join(base_path, \"data\", \"test.csv\")\n",
    "model_path = os.path.join(base_path, \"ridge_price_model.pkl\")\n",
    "submission_path = os.path.join(base_path, \"submission.csv\")\n",
    "\n",
    "# =============================================================\n",
    "#  FUNCTIONS\n",
    "# =============================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def smape_0_100(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
    "    diff = np.abs(y_pred - y_true) / np.maximum(denominator, 1e-8)\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "def get_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"word_count\"] = df[\"catalog_content\"].apply(lambda x: len(x.split()))\n",
    "    df[\"char_count\"] = df[\"catalog_content\"].apply(len)\n",
    "    df[\"digit_count\"] = df[\"catalog_content\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "    df[\"has_price\"] = df[\"catalog_content\"].apply(lambda x: 1 if re.search(r'\\d{3,}', x) else 0)\n",
    "    df[\"price_hint\"] = df[\"catalog_content\"].apply(\n",
    "        lambda x: 1 if any(w in x for w in ['rs', 'price', 'only', 'buy', 'deal']) else 0\n",
    "    )\n",
    "    return df[[\"word_count\", \"char_count\", \"digit_count\", \"has_price\", \"price_hint\"]]\n",
    "\n",
    "# =============================================================\n",
    "#  LOAD DATA\n",
    "# =============================================================\n",
    "print(\"üìÇ Loading training data...\")\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "train_df = train_df[train_df[\"price\"] <= 75000].copy()\n",
    "print(f\"‚úÖ After capping at 75,000: {len(train_df)} rows\")\n",
    "\n",
    "# Winsorize\n",
    "p99 = train_df[\"price\"].quantile(0.99)\n",
    "train_df[\"price\"] = np.clip(train_df[\"price\"], None, p99)\n",
    "print(f\"‚úÖ Winsorized at ${p99:.2f}\")\n",
    "\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "# =============================================================\n",
    "#  SPLIT\n",
    "# =============================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"catalog_content\"],\n",
    "    train_df[\"price\"],\n",
    "    test_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "#  TF-IDF (REDUCED DIMENSION)\n",
    "# =============================================================\n",
    "print(\"üî† Vectorizing text (25k features)...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=25000,        # Reduced for memory\n",
    "    ngram_range=(1, 2),        # Faster, less memory\n",
    "    min_df=2,\n",
    "    stop_words=None,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_valid_tfidf = vectorizer.transform(X_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  FEATURE ENGINEERING\n",
    "# =============================================================\n",
    "feat_train = get_features(pd.DataFrame({\"catalog_content\": X_train}))\n",
    "feat_valid = get_features(pd.DataFrame({\"catalog_content\": X_valid}))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train_scaled = scaler.fit_transform(feat_train)\n",
    "feat_valid_scaled = scaler.transform(feat_valid)\n",
    "\n",
    "# Combine\n",
    "X_train_final = sparse.hstack([X_train_tfidf, feat_train_scaled])\n",
    "X_valid_final = sparse.hstack([X_valid_tfidf, feat_valid_scaled])\n",
    "\n",
    "# Log transform\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)\n",
    "\n",
    "# =============================================================\n",
    "#  GRID SEARCH RIDGE\n",
    "# =============================================================\n",
    "print(\"‚öôÔ∏è GridSearching Ridge...\")\n",
    "ridge = Ridge(max_iter=2000)\n",
    "params = {\"alpha\": [0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "gs = GridSearchCV(ridge, params, cv=5, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "gs.fit(X_train_final, y_train_log)\n",
    "\n",
    "print(\"‚úÖ Best alpha:\", gs.best_params_[\"alpha\"])\n",
    "\n",
    "# =============================================================\n",
    "#  VALIDATION\n",
    "# =============================================================\n",
    "y_pred_valid_log = gs.predict(X_valid_final)\n",
    "y_pred_valid = np.expm1(y_pred_valid_log)\n",
    "y_pred_valid = np.clip(y_pred_valid, 0, 75000)\n",
    "\n",
    "smape_score = smape_0_100(y_valid, y_pred_valid)\n",
    "print(f\"‚úÖ SMAPE (0‚Äì100): {smape_score:.2f}%\")\n",
    "\n",
    "# =============================================================\n",
    "#  SAVE MODEL\n",
    "# =============================================================\n",
    "joblib.dump({\n",
    "    \"model\": gs,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"scaler\": scaler,\n",
    "    \"p99\": p99\n",
    "}, model_path)\n",
    "print(f\"\\nüíæ Model saved: {model_path}\")\n",
    "\n",
    "# =============================================================\n",
    "#  INFERENCE ON TEST\n",
    "# =============================================================\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "test_df[\"catalog_content\"] = test_df[\"catalog_content\"].apply(clean_text)\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(test_df[\"catalog_content\"])\n",
    "feat_test = get_features(test_df)\n",
    "feat_test_scaled = scaler.transform(feat_test)\n",
    "X_test_final = sparse.hstack([X_test_tfidf, feat_test_scaled])\n",
    "\n",
    "preds_log = gs.predict(X_test_final)\n",
    "preds = np.expm1(preds_log)\n",
    "preds = np.clip(preds, 0, 75000)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved: {submission_path}\")\n",
    "print(f\"‚úÖ Max predicted price: {preds.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4037ba3-f425-4c6e-810e-c99c8ed93246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
